<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>用GPT搭建问答系统</title>
<meta name="author" content="dirtysalt" />
<meta name="generator" content="Org Mode" />
<link rel="shortcut icon" href="/themes/favicon.ico" /><link rel="stylesheet" type="text/css" href="/themes/simple.css"/></head>
<body>
<div id="content" class="content">
<h1 class="title">用GPT搭建问答系统</h1>
<p>
一直没有想清楚怎么使用GPT搭建问答系统，我大概知道要做docs和query embedding, 但是不知道匹配到了文档之后，怎么把文档解释出来。
</p>

<p>
之前同事推荐看看这个项目来着 <a href="https://github.com/liaokongVFX/LangChain-Chinese-Getting-Started-Guide">liaokongVFX/LangChain-Chinese-Getting-Started-Guide: LangChain 的中文入门教程</a>, 一直没有时间看，里面很多细节也想不明白。
</p>

<p>
今天在看这个视频 <a href="https://www.youtube.com/watch?v=vw-KWfKwvTQ">GPT-4 - How does it work, and how do I build apps with it? - CS50 Tech Talk - YouTube</a> 里面也提到了问答系统，所以就顺便找找了资源。
</p>

<p>
想灌入文档，第一个想法就是做fine-tunning. 但是这条路不好走，而且很可能是错误的路。 <a href="https://www.youtube.com/watch?v=9qq6HTr7Ocw">OpenAI Q&amp;A: Finetuning GPT-3 vs Semantic Search - which to use, when, and why? - YouTube</a>. fune-tunning几乎是需要训练一个全新任务，而问答系统本质上和GPT任务是一样的，所以其实没有必要做fine-tunning. embedding search才是正确的方法。
</p>

<p>
我理解doc可以拆分成为很多小段(paragraph), 然后每个P单独做embedding存放在vectordb下面，然后query也去做embedding，找个比较相似的paragraphs. 问题是这些小段怎么拼接起来呢？毕竟每个小段只是文档的单独片段（甚至如果简单拆分的话，可能并不是完整的句子）。后面看了这个视频 <a href="https://www.youtube.com/watch?v=es8e4SEuvV0">Answer complex questions from an arbitrarily large set of documents with vector search and GPT-3 - YouTube</a> 大致搞清楚了怎么做。
</p>

<p>
我们在得到许多小段之后，可以重新拼接成为一个prompt来问chatgpt，比如下面这样的格式
</p>

<pre class="example" id="org3e5ff6d">
Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.

&lt;&lt;PARAGRAPHS&gt;&gt;

Question: &lt;&lt;QUERY&gt;&gt;
Helpful Answer:
</pre>

<p>
其实这个在上面github也讲到了
</p>


<blockquote>
<p>
chain 的 chain_type 参数
</p>

<p>
这个参数主要控制了将 document 传递给 llm 模型的方式，一共有 4 种方式：
</p>

<p>
stuff: 这种最简单粗暴，会把所有的 document 一次全部传给 llm 模型进行总结。如果document很多的话，势必会报超出最大 token 限制的错，所以总结文本的时候一般不会选中这个。
</p>

<p>
map_reduce: 这个方式会先将每个 document 进行总结，最后将所有 document 总结出的结果再进行一次总结。
</p>


<div id="org936c5ed" class="figure">
<p><img src="../images/build-ask-system-on-gpt-0.jpg" alt="build-ask-system-on-gpt-0.jpg" />
</p>
</div>

<p>
refine: 这种方式会先总结第一个 document，然后在将第一个 document 总结出的内容和第二个 document 一起发给 llm 模型在进行总结，以此类推。这种方式的好处就是在总结后一个 document 的时候，会带着前一个的 document 进行总结，给需要总结的 document 添加了上下文，增加了总结内容的连贯性。
</p>


<div id="org54dc6b4" class="figure">
<p><img src="../images/build-ask-system-on-gpt-1.jpg" alt="build-ask-system-on-gpt-1.jpg" />
</p>
</div>

<p>
map_rerank: 这种一般不会用在总结的 chain 上，而是会用在问答的 chain 上，他其实是一种搜索答案的匹配方式。首先你要给出一个问题，他会根据问题给每个 document 计算一个这个 document 能回答这个问题的概率分数，然后找到分数最高的那个 document ，在通过把这个 document 转化为问题的 prompt 的一部分（问题+document）发送给 llm 模型，最后 llm 模型返回具体答案。
</p>
</blockquote>

<p>
我也照着github那个参考教程写了一个，就是勉强能跑起来，但是效果不好。我选择的语料是starrocks/docs下面的md文件，同时购买gpt plus得到的效果是还可以的。
</p>

<div class="org-src-container">
<pre class="src src-python"><span class="org-comment-delimiter">#</span><span class="org-comment">!/usr/bin/env python</span>
<span class="org-comment-delimiter"># </span><span class="org-comment">coding: utf-8</span>

<span class="org-py-import-from">from</span> langchain.embeddings.openai <span class="org-py-import-from">import</span> OpenAIEmbeddings
<span class="org-py-import-from">from</span> langchain.vectorstores <span class="org-py-import-from">import</span> Chroma
<span class="org-py-import-from">from</span> langchain.text_splitter <span class="org-py-import-from">import</span> CharacterTextSplitter
<span class="org-py-import-from">from</span> langchain <span class="org-py-import-from">import</span> OpenAI,VectorDBQA
<span class="org-py-import-from">from</span> langchain.document_loaders <span class="org-py-import-from">import</span> DirectoryLoader
<span class="org-py-import-from">from</span> langchain.chains <span class="org-py-import-from">import</span> RetrievalQA
<span class="org-py-import-from">from</span> langchain.chains.question_answering <span class="org-py-import-from">import</span> load_qa_chain

<span class="org-comment-delimiter"># </span><span class="org-comment">&#21152;&#36733;&#25991;&#20214;&#22841;&#20013;&#30340;&#25152;&#26377;txt&#31867;&#22411;&#30340;&#25991;&#20214;</span>
<span class="org-py-variable-name">loader</span> = DirectoryLoader(<span class="org-string">'docs'</span>, <span class="org-py-variable-name">glob</span>=<span class="org-string">'*.md'</span>)
<span class="org-comment-delimiter"># </span><span class="org-comment">&#23558;&#25968;&#25454;&#36716;&#25104; document &#23545;&#35937;&#65292;&#27599;&#20010;&#25991;&#20214;&#20250;&#20316;&#20026;&#19968;&#20010; document</span>
<span class="org-py-variable-name">documents</span> = loader.load()

<span class="org-comment-delimiter"># </span><span class="org-comment">&#21021;&#22987;&#21270;&#21152;&#36733;&#22120;</span>
<span class="org-py-variable-name">text_splitter</span> = CharacterTextSplitter(<span class="org-py-variable-name">chunk_size</span>=<span class="org-py-number">100</span>, <span class="org-py-variable-name">chunk_overlap</span>=<span class="org-py-number">0</span>)
<span class="org-comment-delimiter"># </span><span class="org-comment">&#20999;&#21106;&#21152;&#36733;&#30340; document</span>
<span class="org-py-variable-name">split_docs</span> = text_splitter.split_documents(documents)

<span class="org-comment-delimiter"># </span><span class="org-comment">&#21021;&#22987;&#21270; openai &#30340; embeddings &#23545;&#35937;</span>
<span class="org-py-variable-name">embeddings</span> = OpenAIEmbeddings()<span class="org-py-variable-name">train</span> = <span class="org-py-pseudo-keyword">False</span>
<span class="org-keyword">if</span> train:
    <span class="org-comment"># &#23558; document &#36890;&#36807; openai &#30340; embeddings &#23545;&#35937;&#35745;&#31639; embedding &#21521;&#37327;&#20449;&#24687;&#24182;&#20020;&#26102;&#23384;&#20837; Chroma &#21521;&#37327;&#25968;&#25454;&#24211;&#65292;&#29992;&#20110;&#21518;&#32493;&#21305;&#37197;&#26597;&#35810;</span>
    <span class="org-py-variable-name">docsearch</span> = Chroma.from_documents(split_docs, embeddings, <span class="org-py-variable-name">persist_directory</span>=<span class="org-string">"vector_store"</span>)
    docsearch.persist()
<span class="org-keyword">else</span>:
    <span class="org-py-variable-name">docsearch</span> = Chroma(<span class="org-py-variable-name">persist_directory</span>=<span class="org-string">"vector_store"</span>, <span class="org-py-variable-name">embedding_function</span>=embeddings)

<span class="org-py-def-class">def</span> <span class="org-function-name">ask</span>(query):
    <span class="org-comment"># &#21019;&#24314;&#38382;&#31572;&#23545;&#35937;</span>
    <span class="org-py-variable-name">qa</span> = VectorDBQA.from_chain_type(<span class="org-py-variable-name">llm</span>=OpenAI(), <span class="org-py-variable-name">chain_type</span>=<span class="org-string">"stuff"</span>, <span class="org-py-variable-name">vectorstore</span>=docsearch, <span class="org-py-variable-name">verbose</span> = <span class="org-py-pseudo-keyword">False</span>) <span class="org-comment"># return_source_documents=True)</span>
    <span class="org-comment"># &#36827;&#34892;&#38382;&#31572;</span>
    <span class="org-py-variable-name">result</span> = qa({<span class="org-string">"query"</span>: query})
    <span class="org-keyword">print</span>(<span class="org-string">'===== query ====='</span>)
    <span class="org-keyword">print</span>(result[<span class="org-string">'query'</span>])
    <span class="org-keyword">print</span>(<span class="org-string">'====== result ====='</span>)
    <span class="org-keyword">print</span>(result[<span class="org-string">'result'</span>])

ask(<span class="org-string">"can you give me an sql example of creating primary key table and insert into sql stmt"</span>)
ask(<span class="org-string">"how to stop all routine loads for a database"</span>)
ask(<span class="org-string">"how many joins are supported by starrocks and explain them"</span>)
ask(<span class="org-string">"primary key&#27169;&#22411;&#21644;duplicate key&#27169;&#22411;&#26377;&#20160;&#20040;&#24046;&#21035;&#65311;"</span>)
ask(<span class="org-string">"&#20160;&#20040;&#26102;&#20505;&#24212;&#35813;&#20351;&#29992;&#20027;&#38190;&#27169;&#22411;?"</span>)
</pre>
</div>
</div>
<div id="content"><!-- DISQUS BEGIN --><div id="disqus_thread"></div><script>/***  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/var disqus_config = function () {this.page.url = 'https://dirtysalt.github.io/html/build-ask-system-on-gpt.html';this.page.identifier = 'build-ask-system-on-gpt.html';};(function() {var d = document, s = d.createElement('script');s.src = 'https://dirlt.disqus.com/embed.js';s.setAttribute('data-timestamp', +new Date());(d.head || d.body).appendChild(s);})();</script><noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript><!-- DISQUS END --></div></body>
</html>
